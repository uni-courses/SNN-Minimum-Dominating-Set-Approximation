\chapter{Ideas on the realisation of the algorithm by Alipour et al
in spiking neural networks using Lava}\label{chap:ideas_v1}
List of neurons so far discussed:
\begin{enumerate}
    \item degree_receiver
    \item degree_sender
    \item r (randomly spiking neuron)
    \item w neuron with a spike rate proportional to r + d
    \item wta neurons (a circuit connected to all w neurons from the
    \item neighbourhood of a given vertex)
\end{enumerate}

\section{degree_receiver}\label{sec:degree_receiver}
The weight calculation in the first step can be done by creating a neuron for
every node in the graph. Let this neuron be called degree_sender. Furthermore
create a neuron for every vertex which is called degree_receiver. The
degree_sender neurons are connected to degree_receiver neurons, when the
respective vertices are also connected. We could then calculate the degree of a
neuron by letting all the degree_senders spike and then counting the spikes in
the degree_receiver neuron. Given that we want to keep using this information
however, we need to pass this information on to the next part of the process.
\section{degree_sender}\label{sec:degree_sender}
A rate coding for the above situation might be done like this: the
degree_receiver neurons have a threshold (henceforth called
degree_threshold) higher than the highest degree (but low enough to be time
efficient) and the incoming spikes thus make the neuron spike with a frequency
that relates to the threshold and the degree. whatever comes downstream of
the degree_receiver neuron thus knows about the relative degrees of the
vertices.
\section{r (randomly spiking neuron)}\label{sec:random}
The random number between 0 and 1 that we add thus needs to be added as a
randomly spiking neuron that fires with a chance of between 0 and 100 percent
per timestep. This chance could be set up at the initialisation phase or maybe
introduced by something inherent to loihi or lava (where does randomness in
the brain come from anyway.) Given that the nxsdk and the original lava project
had a built in stochastically spiking neuron, I'm assuming that it will be
implemented in lava as well and until then we can write our own stochastically
spiking process. To make the association of the random spiking neuron and the
randomly sampled value described in the algorithm more clear, we should
create a new neuron called r (for the random spikes) and w (as the result of d
and r in the algorithm ) in which the random spikes and the degree_receiver
spikes come together.
\section{w neuron with a spike rate proportional to r + d}\label{sec:weight_neuron}
We need to find a maximum vertex in the neighbourhood of every vertex. This
can be done using winner take all circuits, but we will have to use a separate
wta-circuit for every vertex which are driven by the w neurons. This seems
excessive, but I'm not sure how to reduce that number of circuits for now. The
wta circuit receives the spikes from the relevant w neurons and using inhibitingsynapses, should select the most spiking neuron. This neuron is spiking (once
per timestep) towards the w neuron of it's associated vertex and is at the same
time inhibiting the degree_receiver neuron. This creates the new spikerate of
the w neuron as in the algorithm.

\section{wta neurons (a circuit connected to all w neurons from the}\label{sec:wta_circuits}

The runtime of the network is dependent on the precise spike rate
implementations. If we choose to use spike rates that are meaningful, they
must be distinguishable for every number represented in the network. This
means that for example the spikerate representing the degree of the highest
degree node and the second highest degree node needs to be different, even
after adding the random value. Thus one possibility to achieve this is to use a
spike rate close to the degree of the highest node, which would result in a
almost constantly spiking w neuron for the node with the highest degree. If this
neuron has a degree of 10, then a node with degree one would only spike at
time 10. thus we would need to measure this spike rate over a window of at
least length 10. For finer measurements, we could introduce longer windows
(not sure yet). However, in this example, the window of a number of spikes
represents one "communication round" as defined in the distributed algorithms
literature. Thus letting the algorithm run multiples of 10 amounts to selecting m
in the original algorithm. Given the precise implementation of these ideas there
might be some "burn in time" or processes that need to start up, which add a
constant amount of time to this, but the main idea should be clear. The wta
circuits are self improving because they after silencing the other neurons they
adjust the weights of their own input and thus allow the wta to select a different
winner. (This needs to be the case and should be possible given the correct
fine tuning of the synapse weights, but I have no proof of it working.)


\section{neighbourhood of a given vertex)}\label{sec:neighbourhood}
At the end, we can attach measuring neurons that never spike to check on the
wta winner neurons at a certain timestep (might need to be a multiple of 10
given the above example)